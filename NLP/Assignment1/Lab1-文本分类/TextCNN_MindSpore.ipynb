{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 导入依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import codecs\n",
    "from pathlib import Path\n",
    "\n",
    "import mindspore\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor\n",
    "from mindspore import context\n",
    "from mindspore.train.model import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "from mindspore.ops import operations as ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "\n",
    "cfg = edict({\n",
    "    'name': 'movie review',\n",
    "    'pre_trained': False,\n",
    "    'num_classes': 2,\n",
    "    'batch_size': 64,\n",
    "    'epoch_size': 4,\n",
    "    'weight_decay': 3e-5,\n",
    "    'data_path': './data/',\n",
    "    'device_target': 'Ascend',\n",
    "    'device_id': 0,\n",
    "    'keep_checkpoint_max': 1,\n",
    "    'checkpoint_path': './ckpt/train_textcnn-4_149.ckpt',\n",
    "    'word_len': 51,\n",
    "    'vec_length': 40\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.set_context(mode=context.GRAPH_MODE, device_target=cfg.device_target, device_id=cfg.device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative reivews:\n",
      "[0]:simplistic , silly and tedious . \n",
      "\n",
      "[1]:it's so laddish and juvenile , only teenage boys could possibly find it funny . \n",
      "\n",
      "[2]:exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n",
      "\n",
      "[3]:[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n",
      "\n",
      "[4]:a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n",
      "\n",
      "Positive reivews:\n",
      "[0]:the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      "[1]:the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n",
      "\n",
      "[2]:effective but too-tepid biopic\n",
      "\n",
      "[3]:if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n",
      "\n",
      "[4]:emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 数据预览\n",
    "with open(\"./data/rt-polarity.neg\", 'r', encoding='utf-8') as f:\n",
    "        print(\"Negative reivews:\")\n",
    "        for i in range(5):\n",
    "            print(\"[{0}]:{1}\".format(i,f.readline()))\n",
    "with open(\"./data/rt-polarity.pos\", 'r', encoding='utf-8') as f:\n",
    "        print(\"Positive reivews:\")\n",
    "        for i in range(5):\n",
    "            print(\"[{0}]:{1}\".format(i,f.readline()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    def __init__(self, input_list):\n",
    "        self.input_list=input_list\n",
    "    def __getitem__(self,item):\n",
    "        return (np.array(self.input_list[item][0],dtype=np.int32),\n",
    "                np.array(self.input_list[item][1],dtype=np.int32))\n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "\n",
    "\n",
    "class MovieReview:\n",
    "    '''\n",
    "    影评数据集\n",
    "    '''\n",
    "    def __init__(self, root_dir, maxlen, split):\n",
    "        '''\n",
    "        input:\n",
    "            root_dir: 影评数据目录\n",
    "            maxlen: 设置句子最大长度\n",
    "            split: 设置数据集中训练/评估的比例\n",
    "        '''\n",
    "        self.path = root_dir\n",
    "        self.feelMap = {\n",
    "            'neg':0,\n",
    "            'pos':1\n",
    "        }\n",
    "        self.files = []\n",
    "\n",
    "        self.doConvert = False\n",
    "        \n",
    "        mypath = Path(self.path)\n",
    "        if not mypath.exists() or not mypath.is_dir():\n",
    "            print(\"please check the root_dir!\")\n",
    "            raise ValueError\n",
    "\n",
    "        # 在数据目录中找到文件\n",
    "        for root,_,filename in os.walk(self.path):\n",
    "            for each in filename:\n",
    "                self.files.append(os.path.join(root,each))\n",
    "            break\n",
    "\n",
    "        # 确认是否为两个文件.neg与.pos\n",
    "        if len(self.files) != 2:\n",
    "            print(\"There are {} files in the root_dir\".format(len(self.files)))\n",
    "            raise ValueError\n",
    "\n",
    "        # 读取数据\n",
    "        self.word_num = 0\n",
    "        self.maxlen = 0\n",
    "        self.minlen = float(\"inf\")\n",
    "        self.maxlen = float(\"-inf\")\n",
    "        self.Pos = []\n",
    "        self.Neg = []\n",
    "        for filename in self.files:\n",
    "            f = codecs.open(filename, 'r')\n",
    "            ff = f.read()\n",
    "            file_object = codecs.open(filename, 'w', 'utf-8')\n",
    "            file_object.write(ff)\n",
    "            self.read_data(filename)\n",
    "        self.PosNeg = self.Pos + self.Neg\n",
    "\n",
    "        self.text2vec(maxlen=maxlen)\n",
    "        self.split_dataset(split=split)\n",
    "\n",
    "    def read_data(self, filePath):\n",
    "\n",
    "        with open(filePath,'r') as f:\n",
    "            \n",
    "            for sentence in f.readlines():\n",
    "                sentence = sentence.replace('\\n','')\\\n",
    "                                    .replace('\"','')\\\n",
    "                                    .replace('\\'','')\\\n",
    "                                    .replace('.','')\\\n",
    "                                    .replace(',','')\\\n",
    "                                    .replace('[','')\\\n",
    "                                    .replace(']','')\\\n",
    "                                    .replace('(','')\\\n",
    "                                    .replace(')','')\\\n",
    "                                    .replace(':','')\\\n",
    "                                    .replace('--','')\\\n",
    "                                    .replace('-',' ')\\\n",
    "                                    .replace('\\\\','')\\\n",
    "                                    .replace('0','')\\\n",
    "                                    .replace('1','')\\\n",
    "                                    .replace('2','')\\\n",
    "                                    .replace('3','')\\\n",
    "                                    .replace('4','')\\\n",
    "                                    .replace('5','')\\\n",
    "                                    .replace('6','')\\\n",
    "                                    .replace('7','')\\\n",
    "                                    .replace('8','')\\\n",
    "                                    .replace('9','')\\\n",
    "                                    .replace('`','')\\\n",
    "                                    .replace('=','')\\\n",
    "                                    .replace('$','')\\\n",
    "                                    .replace('/','')\\\n",
    "                                    .replace('*','')\\\n",
    "                                    .replace(';','')\\\n",
    "                                    .replace('<b>','')\\\n",
    "                                    .replace('%','')\n",
    "                sentence = sentence.split(' ')\n",
    "                sentence = list(filter(lambda x: x, sentence))\n",
    "                if sentence:\n",
    "                    self.word_num += len(sentence)\n",
    "                    self.maxlen = self.maxlen if self.maxlen >= len(sentence) else len(sentence)\n",
    "                    self.minlen = self.minlen if self.minlen <= len(sentence) else len(sentence)\n",
    "                    if 'pos' in filePath:\n",
    "                        self.Pos.append([sentence,self.feelMap['pos']])\n",
    "                    else:\n",
    "                        self.Neg.append([sentence,self.feelMap['neg']])\n",
    "\n",
    "    def text2vec(self, maxlen):\n",
    "        '''\n",
    "        将句子转化为向量\n",
    "\n",
    "        '''\n",
    "        # Vocab = {word : index}\n",
    "        self.Vocab = dict()\n",
    "\n",
    "        # self.Vocab['None']\n",
    "        for SentenceLabel in self.Pos+self.Neg:\n",
    "            vector = [0]*maxlen\n",
    "            for index, word in enumerate(SentenceLabel[0]):\n",
    "                if index >= maxlen:\n",
    "                    break\n",
    "                if word not in self.Vocab.keys():\n",
    "                    self.Vocab[word] = len(self.Vocab)\n",
    "                    vector[index] = len(self.Vocab) - 1\n",
    "                else:\n",
    "                    vector[index] = self.Vocab[word]\n",
    "            SentenceLabel[0] = vector\n",
    "        self.doConvert = True\n",
    "\n",
    "    def split_dataset(self, split):\n",
    "        '''\n",
    "        分割为训练集与测试集\n",
    "\n",
    "        '''\n",
    "\n",
    "        trunk_pos_size = math.ceil((1-split)*len(self.Pos))\n",
    "        trunk_neg_size = math.ceil((1-split)*len(self.Neg))\n",
    "        trunk_num = int(1/(1-split))\n",
    "        pos_temp=list()\n",
    "        neg_temp=list()\n",
    "        for index in range(trunk_num):\n",
    "            pos_temp.append(self.Pos[index*trunk_pos_size:(index+1)*trunk_pos_size])\n",
    "            neg_temp.append(self.Neg[index*trunk_neg_size:(index+1)*trunk_neg_size])\n",
    "        self.test = pos_temp.pop(2)+neg_temp.pop(2)\n",
    "        self.train = [i for item in pos_temp+neg_temp for i in item]\n",
    "\n",
    "        random.shuffle(self.train)\n",
    "        # random.shuffle(self.test)\n",
    "\n",
    "    def get_dict_len(self):\n",
    "        '''\n",
    "        获得数据集中文字组成的词典长度\n",
    "        '''\n",
    "        if self.doConvert:\n",
    "            return len(self.Vocab)\n",
    "        else:\n",
    "            print(\"Haven't finished Text2Vec\")\n",
    "            return -1\n",
    "\n",
    "    def create_train_dataset(self, epoch_size, batch_size):\n",
    "        dataset = ds.GeneratorDataset(\n",
    "                                        source=Generator(input_list=self.train), \n",
    "                                        column_names=[\"data\",\"label\"], \n",
    "                                        shuffle=False\n",
    "                                        )\n",
    "#         dataset.set_dataset_size(len(self.train))\n",
    "        dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n",
    "        dataset=dataset.repeat(epoch_size)\n",
    "        return dataset\n",
    "\n",
    "    def create_test_dataset(self, batch_size):\n",
    "        dataset = ds.GeneratorDataset(\n",
    "                                        source=Generator(input_list=self.test), \n",
    "                                        column_names=[\"data\",\"label\"], \n",
    "                                        shuffle=False\n",
    "                                        )\n",
    "#         dataset.set_dataset_size(len(self.test))\n",
    "        dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = MovieReview(root_dir=cfg.data_path, maxlen=cfg.word_len, split=0.9)\n",
    "dataset = instance.create_train_dataset(batch_size=cfg.batch_size,epoch_size=cfg.epoch_size)\n",
    "batch_num = dataset.get_dataset_size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:18848\n",
      "{'data': Tensor(shape=[64, 51], dtype=Int32, value=\n",
      "[[   62,     0,  7150 ...     0,     0,     0],\n",
      " [    0,    77,     2 ...     0,     0,     0],\n",
      " [  145, 14763,  5254 ...     0,     0,     0],\n",
      " ...\n",
      " [ 1283,  9200,   829 ...     0,     0,     0],\n",
      " [ 5978, 13438,     0 ...     0,     0,     0],\n",
      " [11145,     4,     0 ...     0,     0,     0]]), 'label': Tensor(shape=[64], dtype=Int32, value= [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, \n",
      " 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, \n",
      " 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0])}\n",
      "[    0    77     2 10179    32     0  3621 13677   118  1603  6812  2710\n",
      "    32  8925   118 15183   246    72  6566    72     0  1166    10  6862\n",
      "   246    72  1098    72     0  2946     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "vocab_size=instance.get_dict_len()\n",
    "print(\"vocab_size:{0}\".format(vocab_size))\n",
    "item =dataset.create_dict_iterator()\n",
    "for i,data in enumerate(item):\n",
    "    if i<1:\n",
    "        print(data)\n",
    "        print(data['data'][1])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1训练参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = []\n",
    "warm_up = [1e-3 / math.floor(cfg.epoch_size / 5) * (i + 1) for _ in range(batch_num) \n",
    "           for i in range(math.floor(cfg.epoch_size / 5))]\n",
    "shrink = [1e-3 / (16 * (i + 1)) for _ in range(batch_num) \n",
    "          for i in range(math.floor(cfg.epoch_size * 3 / 5))]\n",
    "normal_run = [1e-3 for _ in range(batch_num) for i in \n",
    "              range(cfg.epoch_size - math.floor(cfg.epoch_size / 5) \n",
    "                    - math.floor(cfg.epoch_size * 2 / 5))]\n",
    "learning_rate = learning_rate + warm_up + normal_run + shrink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weight_variable(shape, factor=0.01):\n",
    "    init_value = np.random.randn(*shape).astype(np.float32) * factor\n",
    "    return Tensor(init_value)\n",
    "\n",
    "\n",
    "def make_conv_layer(kernel_size):\n",
    "    weight_shape = (96, 1, *kernel_size)\n",
    "    weight = _weight_variable(weight_shape)\n",
    "    return nn.Conv2d(in_channels=1, out_channels=96, kernel_size=kernel_size, padding=1,\n",
    "                     pad_mode=\"pad\", weight_init=weight, has_bias=True)\n",
    "\n",
    "\n",
    "class TextCNN(nn.Cell):\n",
    "    def __init__(self, vocab_len, word_len, num_classes, vec_length):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.vec_length = vec_length\n",
    "        self.word_len = word_len\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.unsqueeze = ops.ExpandDims()\n",
    "        self.embedding = nn.Embedding(vocab_len, self.vec_length, embedding_table='normal')\n",
    "\n",
    "        self.slice = ops.Slice()\n",
    "        self.layer1 = self.make_layer(kernel_height=3)\n",
    "        self.layer2 = self.make_layer(kernel_height=4)\n",
    "        self.layer3 = self.make_layer(kernel_height=5)\n",
    "\n",
    "        self.concat = ops.Concat(1)\n",
    "\n",
    "        self.fc = nn.Dense(96*3, self.num_classes)\n",
    "        self.drop = nn.Dropout(keep_prob=0.5)\n",
    "        self.print = ops.Print()\n",
    "        self.reducemean = ops.ReduceMax(keep_dims=False)\n",
    "        \n",
    "    def make_layer(self, kernel_height):\n",
    "        return nn.SequentialCell(\n",
    "            [\n",
    "                make_conv_layer((kernel_height,self.vec_length)),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(self.word_len-kernel_height+1,1)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def construct(self,x):\n",
    "        x = self.unsqueeze(x, 1)\n",
    "        x = self.embedding(x)\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x)\n",
    "        x3 = self.layer3(x)\n",
    "\n",
    "        x1 = self.reducemean(x1, (2, 3))\n",
    "        x2 = self.reducemean(x2, (2, 3))\n",
    "        x3 = self.reducemean(x3, (2, 3))\n",
    "\n",
    "        x = self.concat((x1, x2, x3))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TextCNN(vocab_len=instance.get_dict_len(), word_len=cfg.word_len, \n",
    "              num_classes=cfg.num_classes, vec_length=cfg.vec_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN<\n",
      "  (embedding): Embedding<vocab_size=18848, embedding_size=40, use_one_hot=False, embedding_table=Parameter (name=embedding.embedding_table, shape=(18848, 40), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
      "  (layer1): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(3, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[-5.65948244e-03  1.31873023e-02 -6.67610951e-03 ... -1.49690302e-03\n",
      "         6.13702647e-03 -2.21290858e-03]\n",
      "       [ 3.63341137e-03  1.51631981e-02  1.14100997e-03 ...  1.00660687e-02\n",
      "        -3.33538446e-05  1.17512103e-02]\n",
      "       [ 1.58199982e-03  1.40077469e-03 -9.91982408e-03 ...  8.66411999e-03\n",
      "        -2.42299261e-03  3.72373592e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[ 8.01048125e-04  1.80296581e-02  1.12059088e-02 ... -1.44267091e-04\n",
      "         1.05663380e-02  3.20006371e-03]\n",
      "       [ 4.89957444e-03 -2.04381198e-02 -1.68501921e-02 ... -1.04028583e-02\n",
      "        -9.11329465e-04 -5.21812937e-04]\n",
      "       [-7.46831065e-03 -9.00168717e-03 -1.07622147e-02 ...  1.07035006e-03\n",
      "        -1.12759145e-02 -7.37136754e-04]]]\n",
      "    \n",
      "    \n",
      "     [[[ 3.14426096e-03  5.78014646e-03  4.94142948e-03 ...  4.94450051e-03\n",
      "         2.50803190e-03  6.04020339e-03]\n",
      "       [-5.60410973e-03  1.83750726e-02  2.29661223e-02 ...  2.11465871e-03\n",
      "        -2.20999885e-02  2.26215948e-03]\n",
      "       [-1.02952933e-02 -1.40099321e-02  9.52539314e-03 ... -1.07922098e-02\n",
      "         3.56656848e-03  2.63878494e-04]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[ 3.47656221e-03  3.93876573e-03 -1.32217593e-02 ...  1.09934984e-02\n",
      "         1.01061473e-02  2.27440801e-02]\n",
      "       [ 1.04507096e-02 -4.77643451e-04  4.26129252e-03 ... -7.67949503e-03\n",
      "         7.27381930e-03  6.37303386e-03]\n",
      "       [-7.99459592e-03  6.60016155e-03  5.20923315e-03 ...  1.38264894e-02\n",
      "        -7.86219072e-03 -1.81657281e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[-6.55154977e-03  2.01739851e-04 -1.16245076e-02 ... -1.92047248e-03\n",
      "         4.48571425e-03 -1.46270441e-02]\n",
      "       [-1.29680222e-04  3.46333114e-03 -5.72521705e-04 ... -9.02917876e-04\n",
      "         8.18073563e-03 -4.17782646e-03]\n",
      "       [ 7.81616475e-03  9.15697135e-04 -2.38194084e-03 ... -5.20618260e-03\n",
      "        -3.49814561e-03 -1.16283176e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[-1.06249060e-02  5.71985589e-03 -1.54971695e-02 ... -9.18061659e-03\n",
      "         1.04823234e-02 -1.83435511e-02]\n",
      "       [-1.20757823e-03 -7.64191290e-03 -2.17587478e-03 ... -4.93480777e-03\n",
      "        -3.17258202e-03 -3.79933766e-03]\n",
      "       [ 2.66395463e-03  1.33766956e-03 -1.42893093e-02 ...  1.61344539e-02\n",
      "         1.31203502e-03 -7.57681392e-03]]]], bias_init=zeros, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(49, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (layer2): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(4, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[-1.91786804e-03 -6.55959966e-03  9.09749046e-03 ...  1.22118173e-02\n",
      "        -1.69933308e-02 -8.61507002e-03]\n",
      "       [-9.47937649e-03  7.39265420e-03  6.56114006e-03 ...  9.25810309e-04\n",
      "        -9.95506067e-03  9.08358488e-05]\n",
      "       [-6.08060881e-03 -1.53353009e-02 -6.11586787e-04 ... -2.15672120e-03\n",
      "        -5.67477150e-03 -1.56988390e-02]\n",
      "       [-6.88481005e-03  1.09959964e-03  3.61675280e-03 ... -1.01451911e-02\n",
      "        -6.80555473e-04  5.69875876e-04]]]\n",
      "    \n",
      "    \n",
      "     [[[-4.06268239e-03  6.80560851e-03  1.89355258e-02 ... -3.82665964e-03\n",
      "         1.60882261e-03 -6.82360027e-03]\n",
      "       [ 2.43287417e-03  9.68952943e-03  7.38599710e-03 ... -7.38316623e-04\n",
      "         1.15017891e-02  4.60340409e-03]\n",
      "       [-1.00281155e-02 -1.78361833e-02 -9.52148438e-03 ... -5.85280405e-03\n",
      "         4.48628794e-03  3.51633914e-02]\n",
      "       [ 3.35858902e-03  1.20891212e-02 -3.42269591e-03 ... -4.93701315e-03\n",
      "         1.22328434e-04 -2.13189796e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[ 1.50716472e-02  5.27924858e-04 -3.55374021e-03 ... -1.65010765e-02\n",
      "         1.02349021e-03 -2.17338488e-03]\n",
      "       [-1.65303014e-02 -1.53834689e-02 -1.11600431e-02 ... -1.40435891e-02\n",
      "         2.22324030e-04  6.39868202e-03]\n",
      "       [ 4.04540962e-03  1.42989159e-02 -4.00606450e-03 ... -3.53508978e-03\n",
      "         5.76355215e-03 -5.68347191e-03]\n",
      "       [-4.84379148e-03 -1.43673318e-03 -3.13570909e-02 ... -8.85468069e-03\n",
      "         1.53481157e-03  1.41806416e-02]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[ 6.41180901e-03 -1.21315429e-02 -1.88045227e-03 ...  4.58623841e-03\n",
      "         6.28944207e-03  4.67701023e-03]\n",
      "       [ 3.84668051e-03 -6.72546588e-03  4.08165110e-03 ...  1.13420412e-02\n",
      "         7.35075446e-03  4.43229917e-03]\n",
      "       [-2.44174828e-03 -2.00406765e-03  1.38029680e-02 ...  1.54087869e-02\n",
      "         9.58883017e-03 -3.46369296e-03]\n",
      "       [-6.17610174e-04  3.12033924e-03  7.49350432e-03 ...  9.20197275e-03\n",
      "         9.73026175e-03  2.39964072e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[-1.90986751e-03  3.69624840e-03  7.05276243e-03 ... -1.37431789e-02\n",
      "        -7.48406909e-03  2.86601647e-03]\n",
      "       [ 4.99700254e-04 -6.96026720e-03  1.21461949e-03 ... -1.78597670e-03\n",
      "        -3.96908727e-03  5.03745349e-03]\n",
      "       [ 2.54265056e-03  1.81136723e-03  2.08343007e-02 ...  5.61894209e-04\n",
      "        -5.35906060e-03 -6.95833052e-03]\n",
      "       [ 2.87297624e-03 -6.49369601e-03 -8.42282362e-03 ... -1.07384729e-03\n",
      "         9.29951854e-03 -5.19198691e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-1.29828998e-03  9.29265842e-03  3.22530419e-03 ...  5.57534862e-03\n",
      "        -3.26304394e-03  3.62709077e-04]\n",
      "       [ 2.96749844e-04 -1.48127386e-02 -7.13555934e-03 ... -5.15626650e-03\n",
      "         7.42867077e-03  4.41352930e-03]\n",
      "       [ 7.18653714e-03  1.01386392e-02  9.33124320e-05 ... -5.84606873e-03\n",
      "         9.64606740e-03 -7.49841845e-03]\n",
      "       [ 8.96333531e-03 -8.65694135e-03  1.32196443e-02 ... -3.91795812e-03\n",
      "         5.40578076e-05 -2.68082158e-03]]]], bias_init=zeros, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(48, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (layer3): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(5, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[ 3.00004054e-03  6.64091145e-04 -6.88445754e-04 ... -1.74389761e-02\n",
      "        -6.33881567e-03 -2.02944130e-02]\n",
      "       [-1.94597058e-02  1.29163060e-02 -1.59786288e-02 ... -1.03527093e-02\n",
      "         1.78130483e-03  7.70575367e-03]\n",
      "       [ 1.74902957e-02  2.42066737e-02  8.98290705e-03 ...  8.44133552e-03\n",
      "         4.57398547e-03  1.06854709e-02]\n",
      "       [ 3.05947359e-03 -5.11914911e-03 -9.98179335e-03 ... -2.20655114e-03\n",
      "        -9.45054577e-04 -3.30250384e-03]\n",
      "       [-1.48853986e-02 -4.39871219e-04 -1.16129238e-02 ...  3.53599014e-03\n",
      "        -2.23604147e-03 -6.07959600e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-9.55206715e-03  3.14827636e-03  9.63827129e-03 ... -2.95969984e-03\n",
      "         4.60750284e-03 -4.40528290e-03]\n",
      "       [ 1.29969846e-02  7.89696630e-03 -1.10693453e-02 ... -4.42781160e-03\n",
      "         9.52107925e-03 -4.51810920e-04]\n",
      "       [-1.36097623e-02  1.25546623e-02  5.52151073e-03 ...  4.39397478e-03\n",
      "         1.16198417e-02  1.20497094e-02]\n",
      "       [-5.64964395e-03 -1.35571288e-03 -5.98925212e-03 ...  6.77946862e-03\n",
      "        -2.34717940e-04 -8.56025238e-03]\n",
      "       [-3.45337414e-03 -9.41021554e-03 -8.70145764e-03 ...  9.42039490e-03\n",
      "         8.67161714e-03  2.25360319e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[ 3.36743053e-03 -1.99964084e-02  2.09910632e-03 ... -4.69740480e-03\n",
      "        -4.94983187e-03 -3.21512204e-03]\n",
      "       [ 7.88132846e-03  1.55383931e-03 -9.23749711e-03 ...  9.05849505e-03\n",
      "         1.40035129e-03  4.38970607e-03]\n",
      "       [ 6.18004054e-03 -3.37509741e-03 -1.66274291e-02 ... -1.53222319e-03\n",
      "         5.50871016e-03 -1.55256959e-02]\n",
      "       [-6.54166378e-03  1.51406871e-02 -6.65400736e-03 ... -1.58710417e-03\n",
      "         4.91273496e-03 -7.29631539e-03]\n",
      "       [-2.12213933e-03  6.42707432e-03 -3.89337679e-03 ... -7.65448622e-03\n",
      "         1.08282529e-02 -3.23526724e-03]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[-1.82584897e-02 -1.33115309e-03 -4.73522209e-03 ...  9.14189871e-03\n",
      "        -1.96405016e-02  7.72425113e-03]\n",
      "       [-1.49808370e-03 -1.67651428e-03 -3.13566998e-03 ... -1.29834376e-02\n",
      "        -5.13617229e-03  5.82554843e-03]\n",
      "       [-5.44565311e-03 -9.68513917e-03 -3.11880186e-03 ... -7.58638792e-03\n",
      "        -1.79595463e-02 -1.85953468e-04]\n",
      "       [ 2.00614217e-03  6.77139033e-03 -1.04058105e-02 ... -1.81567203e-02\n",
      "        -2.08482761e-02 -1.12279998e-02]\n",
      "       [-5.85173909e-03  1.58163290e-02 -1.06296157e-02 ...  8.18403438e-03\n",
      "        -1.82879642e-02  1.33990236e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[-1.22434162e-02 -3.77487834e-03  3.50239826e-03 ...  3.44214309e-03\n",
      "         8.09178408e-03 -2.67633647e-02]\n",
      "       [ 7.71180028e-03  1.90219954e-02 -2.13962235e-02 ...  2.71313055e-03\n",
      "         3.83094623e-04  2.11013630e-02]\n",
      "       [ 1.57962851e-02 -1.53383194e-02  1.56555080e-03 ...  1.55655609e-03\n",
      "        -2.42611044e-03  2.65042223e-02]\n",
      "       [ 1.06114813e-03  1.68291274e-02 -2.12495346e-02 ...  3.55177838e-03\n",
      "        -1.78119764e-02 -3.62756313e-03]\n",
      "       [-1.44344950e-02  5.29291807e-03  6.09752582e-03 ...  8.16837233e-03\n",
      "        -6.09740941e-03  1.03391325e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[ 9.71559249e-03  1.29254973e-02 -8.05639941e-03 ... -2.36100703e-03\n",
      "         8.62254947e-03  9.82934725e-04]\n",
      "       [-5.22592338e-03 -3.97891551e-03  2.73943674e-02 ...  9.20324121e-03\n",
      "         6.34980388e-04  5.98556269e-03]\n",
      "       [-2.67361151e-03  7.80043891e-03  9.57349967e-03 ... -3.51339509e-03\n",
      "         1.41694152e-03  1.21484185e-02]\n",
      "       [ 1.44084729e-03 -1.41992206e-02  1.72245502e-02 ... -4.41188272e-03\n",
      "        -2.14830413e-02 -7.09048007e-03]\n",
      "       [-5.60631597e-05 -2.95587350e-03 -4.00219718e-03 ...  8.50101281e-03\n",
      "         4.41926811e-03  5.17527619e-03]]]], bias_init=zeros, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(47, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (fc): Dense<input_channels=288, output_channels=2, has_bias=True>\n",
      "  (drop): Dropout<keep_prob=0.5>\n",
      "  >\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training if set pre_trained to be True\n",
    "if cfg.pre_trained:\n",
    "    param_dict = load_checkpoint(cfg.checkpoint_path)\n",
    "    load_param_into_net(net, param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = nn.Adam(filter(lambda x: x.requires_grad, net.get_parameters()), \n",
    "              learning_rate=learning_rate, weight_decay=cfg.weight_decay)\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(net, loss_fn=loss, optimizer=opt, metrics={'acc': Accuracy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_ck = CheckpointConfig(save_checkpoint_steps=int(cfg.epoch_size*batch_num/2), keep_checkpoint_max=cfg.keep_checkpoint_max)\n",
    "time_cb = TimeMonitor(data_size=batch_num)\n",
    "ckpt_save_dir = \"./ckpt\"\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"train_textcnn\", directory=ckpt_save_dir, config=config_ck)\n",
    "loss_cb = LossMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 596, loss is 0.11741524189710617\n",
      "epoch time: 29582.146 ms, per step time: 49.634 ms\n",
      "epoch: 2 step: 596, loss is 0.005207548383623362\n",
      "epoch time: 4846.789 ms, per step time: 8.132 ms\n",
      "epoch: 3 step: 596, loss is 0.0015325902495533228\n",
      "epoch time: 4793.690 ms, per step time: 8.043 ms\n",
      "epoch: 4 step: 596, loss is 0.0007978661451488733\n",
      "epoch time: 4873.424 ms, per step time: 8.177 ms\n",
      "train success\n"
     ]
    }
   ],
   "source": [
    "model.train(cfg.epoch_size, dataset, callbacks=[time_cb, ckpoint_cb, loss_cb])\n",
    "print(\"train success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 测试评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './ckpt/train_textcnn-4_596.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from [./ckpt/train_textcnn-4_596.ckpt].\n",
      "accuracy:  {'acc': 0.7626953125}\n"
     ]
    }
   ],
   "source": [
    "dataset = instance.create_test_dataset(batch_size=cfg.batch_size)\n",
    "opt = nn.Adam(filter(lambda x: x.requires_grad, net.get_parameters()), \n",
    "              learning_rate=0.001, weight_decay=cfg.weight_decay)\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)\n",
    "net = TextCNN(vocab_len=instance.get_dict_len(),word_len=cfg.word_len,\n",
    "                  num_classes=cfg.num_classes,vec_length=cfg.vec_length)\n",
    "\n",
    "if checkpoint_path is not None:\n",
    "    param_dict = load_checkpoint(checkpoint_path)\n",
    "    print(\"load checkpoint from [{}].\".format(checkpoint_path))\n",
    "else:\n",
    "    param_dict = load_checkpoint(cfg.checkpoint_path)\n",
    "    print(\"load checkpoint from [{}].\".format(cfg.checkpoint_path))\n",
    "\n",
    "load_param_into_net(net, param_dict)\n",
    "net.set_train(False)\n",
    "model = Model(net, loss_fn=loss, metrics={'acc': Accuracy()})\n",
    "\n",
    "acc = model.eval(dataset)\n",
    "print(\"accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 在线测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = sentence.replace('\\n','')\\\n",
    "                                    .replace('\"','')\\\n",
    "                                    .replace('\\'','')\\\n",
    "                                    .replace('.','')\\\n",
    "                                    .replace(',','')\\\n",
    "                                    .replace('[','')\\\n",
    "                                    .replace(']','')\\\n",
    "                                    .replace('(','')\\\n",
    "                                    .replace(')','')\\\n",
    "                                    .replace(':','')\\\n",
    "                                    .replace('--','')\\\n",
    "                                    .replace('-',' ')\\\n",
    "                                    .replace('\\\\','')\\\n",
    "                                    .replace('0','')\\\n",
    "                                    .replace('1','')\\\n",
    "                                    .replace('2','')\\\n",
    "                                    .replace('3','')\\\n",
    "                                    .replace('4','')\\\n",
    "                                    .replace('5','')\\\n",
    "                                    .replace('6','')\\\n",
    "                                    .replace('7','')\\\n",
    "                                    .replace('8','')\\\n",
    "                                    .replace('9','')\\\n",
    "                                    .replace('`','')\\\n",
    "                                    .replace('=','')\\\n",
    "                                    .replace('$','')\\\n",
    "                                    .replace('/','')\\\n",
    "                                    .replace('*','')\\\n",
    "                                    .replace(';','')\\\n",
    "                                    .replace('<b>','')\\\n",
    "                                    .replace('%','')\\\n",
    "                                    .replace(\"  \",\" \")\n",
    "    sentence = sentence.split(' ')\n",
    "    maxlen = cfg.word_len\n",
    "    vector = [0]*maxlen\n",
    "    for index, word in enumerate(sentence):\n",
    "        if index >= maxlen:\n",
    "            break\n",
    "        if word not in instance.Vocab.keys():\n",
    "            print(word,\"单词未出现在字典中\")\n",
    "        else:\n",
    "            vector[index] = instance.Vocab[word]\n",
    "    sentence = vector\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def inference(review_en):\n",
    "    review_en = preprocess(review_en)\n",
    "    input_en = Tensor(np.array([review_en]).astype(np.int32))\n",
    "    output = net(input_en)\n",
    "    if np.argmax(np.array(output[0])) == 1:\n",
    "        print(\"Positive comments\")\n",
    "    else:\n",
    "        print(\"Negative comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative comments\n"
     ]
    }
   ],
   "source": [
    "review_en = \"I hope it would be a good film, unfortunately it wasn't.\"\n",
    "inference(review_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
