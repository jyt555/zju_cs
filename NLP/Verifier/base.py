import json
import re
import random
from openai import OpenAI
import time  # æ·»åŠ æ—¶é—´æ¨¡å—ç”¨äºç›‘æ§æ‰§è¡Œæ—¶é—´

# é…ç½®é˜¿é‡Œäº‘ç™¾ç‚¼API
client = OpenAI(api_key="",
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")

# è°ƒç”¨APIå¾—åˆ°å›ç­”
def generate_response(messages, temperature=0.7, max_tokens=1024):
    """é€šè¿‡APIè°ƒç”¨ç”Ÿæˆå›å¤"""
    try:
        start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
        completion = client.chat.completions.create(
            model="qwen3-30b-a3b", 
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            extra_body={"enable_thinking": False}
        )
        elapsed = time.time() - start_time
        # print(f"âœ“ APIè°ƒç”¨æˆåŠŸ | è€—æ—¶: {elapsed:.2f}ç§’ | Token: {max_tokens} | æ¸©åº¦: {temperature}")
        return completion.choices[0].message.content
    except Exception as e:
        print(f"âœ— APIè°ƒç”¨å‡ºé”™: {e}")
        return "APIè°ƒç”¨å¤±è´¥ï¼Œè¯·ç¨åå†è¯•ã€‚"

# ======================== Step-Aware Verifier æ¨¡å— ========================
def split_reasoning_steps(reasoning):
    """å°†æ¨ç†æ–‡æœ¬åˆ†å‰²ä¸ºç‹¬ç«‹æ­¥éª¤"""
    delimiters = r"\n\d+[.\)] |\n- |\nâ€¢ |\n\d+\) |; |\n"
    steps = re.split(delimiters, reasoning)
    return [s.strip() for s in steps if s.strip()]

def verify_step(question, step_content):
    """éªŒè¯å•ä¸ªæ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§ï¼ˆè‹±æ–‡æç¤ºï¼‰"""
    # print(f"ğŸ” éªŒè¯æ¨ç†æ­¥éª¤ | æ­¥éª¤å†…å®¹: {step_content[:50]}...")
    prompt = [
        {
            "role": "system", 
            "content": "You are a strict math teacher evaluating a student's solution step. Please rigorously check the correctness of the following reasoning step:"
        },
        {
            "role": "user",
            "content": f"Question: {question}\nReasoning step: {step_content}\n\n"
                      "Please respond in the following format:\n"
                      "1. Correctness score (0.0-1.0):\n"
                      "2. Error analysis (if applicable):\n"
                      "3. Improvement suggestion (if applicable):"
        }
    ]
    return generate_response(prompt, temperature=0.3, max_tokens=256)

def parse_verification_response(response):
    """è§£æéªŒè¯å™¨è¿”å›çš„å“åº”ï¼ˆè‹±æ–‡å…³é”®è¯ï¼‰"""
    score_match = re.search(r"Correctness score[:\s]*([0-9.]+)", response)
    analysis_match = re.search(r"Error analysis[:\s]*(.+)", response, re.DOTALL)
    suggestion_match = re.search(r"Improvement suggestion[:\s]*(.+)", response, re.DOTALL)
    
    score = float(score_match.group(1)) if score_match else 0.0
    analysis = analysis_match.group(1).strip() if analysis_match else ""
    suggestion = suggestion_match.group(1).strip() if suggestion_match else ""
    
    print(f"  éªŒè¯ç»“æœ | è¯„åˆ†: {score:.2f} | åˆ†æ: {analysis[:50]}{'...' if len(analysis) > 50 else ''}")
    return {
        "score": score,
        "analysis": analysis,
        "suggestion": suggestion
    }

def verify_full_path(question, reasoning_path):
    """éªŒè¯å®Œæ•´æ¨ç†è·¯å¾„"""
    print(f"ğŸ” å¼€å§‹éªŒè¯å®Œæ•´æ¨ç†è·¯å¾„ | è·¯å¾„é•¿åº¦: {len(reasoning_path)}å­—ç¬¦")
    total_score = 0
    valid_steps = 0
    verification_report = []
    
    steps = split_reasoning_steps(reasoning_path)
    print(f"  åˆ†å‰²ä¸º {len(steps)} ä¸ªç‹¬ç«‹æ­¥éª¤")
    
    for i, step in enumerate(steps):
        # print(f"  æ­¥éª¤ {i+1}/{len(steps)} | å†…å®¹: {step[:30]}{'...' if len(step) > 30 else ''}")
        verification = verify_step(question, step)
        parsed = parse_verification_response(verification)
        total_score += parsed["score"]
        valid_steps += 1 if parsed["score"] > 0.7 else 0
        
        verification_report.append({
            "step": i+1,
            "content": step,
            "verification": parsed
        })
    
    path_confidence = total_score / len(steps) if steps else 0
    step_accuracy = valid_steps / len(steps) if steps else 0
    
    print(f"âœ… è·¯å¾„éªŒè¯å®Œæˆ | ç½®ä¿¡åº¦: {path_confidence:.2f} | æ­¥éª¤å‡†ç¡®ç‡: {step_accuracy:.2f}")
    return {
        "confidence": path_confidence,
        "step_accuracy": step_accuracy,
        "report": verification_report
    }

def correct_reasoning(question, original_reasoning, verification):
    """ä¿®æ­£æ¨ç†è·¯å¾„ä¸­çš„é”™è¯¯æ­¥éª¤"""
    error_steps = [s for s in verification["report"] if s["verification"]["score"] < 0.7]
    
    if not error_steps:
        print("ğŸŸ¢ æ— éœ€ä¿®æ­£ - æ‰€æœ‰æ­¥éª¤è¯„åˆ†å‡é«˜äº0.7")
        return original_reasoning
    
    print(f"âš ï¸ éœ€è¦ä¿®æ­£ - {len(error_steps)}ä¸ªä½åˆ†æ­¥éª¤")
    error_info = [{
        "step": step["content"],
        "analysis": step["verification"]["analysis"],
        "suggestion": step["verification"]["suggestion"]
    } for step in error_steps]
    
    prompt = [
        {
            "role": "system",
            "content": "You are correcting errors in mathematical reasoning. Below is the original question, reasoning process, and verification report:"
        },
        {
            "role": "user",
            "content": f"Question: {question}\n"
                      f"Original reasoning:\n{original_reasoning}\n\n"
                      f"Verification report (error steps):\n{json.dumps(error_info, ensure_ascii=False)}\n\n"
                      "Please generate a corrected reasoning process, preserving correct steps and correcting those marked as errors."
        }
    ]
    
    return generate_response(prompt, temperature=0.5, max_tokens=1024)
# ======================== Step-Aware Verifier æ¨¡å—ç»“æŸ ========================

# æ·»åŠ äº†å¤šè·¯å¾„æ¨ç†ã€æ‰¹å¤„ç†å’ŒStep-Aware Verifierçš„CoTæ¨ç†
def cot_Qwen(questions, trigger_phrase="Let's think step by step.", path_num=3, batch_size=16, enable_verifier=True):
    """CoTæ¨ç†å‡½æ•°ï¼ŒåŒ…å«Step-Aware Verifier"""
    print("\n" + "="*60)
    print(f"ğŸš€ å¼€å§‹CoTæ¨ç† | é—®é¢˜æ•°: {len(questions)} | è·¯å¾„æ•°: {path_num} | Verifier: {'å¯ç”¨' if enable_verifier else 'ç¦ç”¨'}")
    print("="*60)
    
    correct = 0
    total = 0
    step1_replies = []
    gold_answers = []
    full_questions = []
    
    # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆæ¨ç†æ­¥éª¤
    for i, q in enumerate(questions): 
        data = json.loads(q.strip())
        question = data['question']
        options = "\n".join(data['options'])
        full_question = f"{question}\nOptions:\n{options}"
        full_questions.append(full_question)
        gold_answers.append(data['correct'])

        step1_prompt = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": f"{full_question} {trigger_phrase}"}
        ]

        # print(f"\nğŸ”„ ç”Ÿæˆæ¨ç†è·¯å¾„ | é—®é¢˜ {i+1}/{len(questions)}: {question[:30]}{'...' if len(question) > 30 else ''}")
        for path_i in range(path_num):
            temperature = 0.7 + 0.3 * path_i
            # print(f"  è·¯å¾„ {path_i+1}/{path_num} | æ¸©åº¦: {temperature:.1f}")
            reply = generate_response(step1_prompt, temperature, max_tokens=1024)
            step1_replies.append(reply)
    
    answers = []
    candidate_lists = []
    
    # ç¬¬äºŒæ­¥ï¼šç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    for idx in range(len(questions)): 
        q = full_questions[idx]
        print(f"\nğŸ” å¤„ç†é—®é¢˜ {idx+1}/{len(questions)}: {q.splitlines()[0][:30]}{'...' if len(q) > 30 else ''}")
        
        candidates_reply = step1_replies[idx * path_num: (idx + 1) * path_num]
        verified_replies = []
        
        # åº”ç”¨Step-Aware Verifier
        for j, reply in enumerate(candidates_reply):
            print(f"  ğŸ”„ å¤„ç†è·¯å¾„ {j+1}/{len(candidates_reply)} | é•¿åº¦: {len(reply)}å­—ç¬¦")
            
            if enable_verifier:
                verification = verify_full_path(q, reply)
                
                if verification["step_accuracy"] < 0.8:
                    print(f"  âš ï¸ è·¯å¾„ {j+1}å‡†ç¡®ç‡ä½ ({verification['step_accuracy']:.2f}) - å°è¯•ä¿®æ­£")
                    corrected_reply = correct_reasoning(q, reply, verification)
                    verified_replies.append({
                        "original": reply,
                        "corrected": corrected_reply,
                        "verification": verification,
                        "is_corrected": True
                    })
                else:
                    print(f"  âœ… è·¯å¾„ {j+1}å‡†ç¡®ç‡è‰¯å¥½ ({verification['step_accuracy']:.2f}) - ç›´æ¥ä½¿ç”¨")
                    verified_replies.append({
                        "original": reply,
                        "corrected": reply,
                        "verification": verification,
                        "is_corrected": False
                    })
            else:
                verified_replies.append({
                    "original": reply,
                    "corrected": reply,
                    "is_corrected": False
                })
        
        # ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        candidates_res = []
        for k, v_reply in enumerate(verified_replies):
            print(f"  ğŸ’¡ ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ | è·¯å¾„ {k+1}/{len(verified_replies)}")
            step2_prompt = [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": f"{q} {trigger_phrase} {v_reply['corrected']} Therefore, among A through E, the answer is"}
            ]
            answer = generate_response(step2_prompt, 0.7, max_tokens=1024)
            candidates_res.append(answer)
        
        final, candidates = get_final_res(candidates_res)
        candidate_lists.append(candidates)
        answers.append(final)
    
    # è¾“å‡ºç»“æœ
    for i, (question, candidate_answers, gold_answer, pred_answer) in enumerate(zip(full_questions, candidate_lists, gold_answers, answers)):
        print(f"\nğŸ“Š ç»“æœ {i+1}/{len(questions)}")
        print(f"  é—®é¢˜: {question.splitlines()[0][:30]}{'...' if len(question) > 30 else ''}")
        print(f"  å€™é€‰ç­”æ¡ˆ: {candidate_answers}")
        print(f"  æ­£ç¡®ç­”æ¡ˆ: {gold_answer}, é¢„æµ‹ç­”æ¡ˆ: {pred_answer}")
        
        if pred_answer and pred_answer.upper() == gold_answer.upper():
            print(f"  âœ… æ­£ç¡®!")
            correct += 1
        else:
            print(f"  âŒ é”™è¯¯!")
        total += 1
        
    print(f"\nğŸ¯ æœ¬æ‰¹æ¬¡å®Œæˆ | æ­£ç¡®: {correct}/{total} | å‡†ç¡®ç‡: {correct/total:.2f}" if total > 0 else "æœ¬æ‰¹æ¬¡æ— æœ‰æ•ˆé—®é¢˜")
    return total, correct

# ä»å€™é€‰çš„å¤šè·¯å¾„æ¨ç†ç»“æœä¸­åŒ¹é…å‡ºå€™é€‰ç»“æœå¹¶é€‰å‡ºå”¯ä¸€æœ€ç»ˆç»“æœ
def get_final_res(candidates_res):
    """ä»å¤šä¸ªç­”æ¡ˆä¸­ç¡®å®šæœ€ç»ˆç­”æ¡ˆ"""
    pred_answers = []
    for candidate in candidates_res:
        pred_answer = None
        if candidate:
            bold_match = re.search(r'\*\*(?:.*?)([A-E])\)[^\*]*\*\*', candidate)
            if bold_match:
                pred_answer = re.search(r'([A-E])\)', bold_match.group()).group(1)
            else:
                simple_match = re.search(r'\b([A-E])\b', candidate)
                if simple_match:
                    pred_answer = simple_match.group(1)
        pred_answers.append(pred_answer)
    
    valid_answers = [ans for ans in pred_answers if ans is not None]
    
    if valid_answers:
        final = max(set(valid_answers), key=valid_answers.count)
        print(f"  æœ€ç»ˆç­”æ¡ˆç¡®å®š: {final} (æ¥è‡ª {valid_answers.count(final)}/{len(valid_answers)} è·¯å¾„)")
        return final, pred_answers
    else:
        print("âš ï¸ æ— æ³•ç¡®å®šæœ€ç»ˆç­”æ¡ˆ - æ‰€æœ‰è·¯å¾„å‡æ— æœ‰æ•ˆç­”æ¡ˆ")
        return None, pred_answers

def evaluate_json_questions_with_api(json_path='AQuA/test.json', max_samples=None, path_num=3, batch_size=16, enable_verifier=True):
    """
    è¯„ä¼°JSONæ ¼å¼çš„é¢˜ç›®æ•°æ®ï¼ŒåŒ…å«Step-Aware Verifieré€‰é¡¹
    """
    print(f"\n{'='*60}\nğŸ” å¼€å§‹è¯„ä¼°æ•°æ®é›† | æ–‡ä»¶: {json_path}\n{'='*60}")
    correct = 0
    total = 0

    with open(json_path, 'r', encoding='utf-8') as f:
        all_data = f.readlines()
    
    random.seed(42)
    if max_samples:
        selected_questions = random.sample(all_data, min(max_samples, len(all_data)))
    else:
        selected_questions = all_data
    
    num_questions = len(selected_questions)
    print(f"ğŸ“‚ æ•°æ®é›†åŠ è½½å®Œæˆ | æ€»é—®é¢˜æ•°: {len(all_data)} | é‡‡æ ·æ•°: {num_questions}")
    print(f"âš™ï¸ é…ç½® | è·¯å¾„æ•°: {path_num} | æ‰¹æ¬¡å¤§å°: {batch_size} | Verifier: {'å¯ç”¨' if enable_verifier else 'ç¦ç”¨'}")

    for batch_idx, start_idx in enumerate(range(0, num_questions, batch_size)):
        batch_q = selected_questions[start_idx: start_idx + batch_size]
        print(f"\n{'='*50}")
        print(f"ğŸ”§ å¤„ç†æ‰¹æ¬¡ {batch_idx+1}/{(num_questions-1)//batch_size+1} | é—®é¢˜: {start_idx+1}-{min(start_idx+batch_size, num_questions)}")
        print(f"{'='*50}")

        try:            
            total_batch, correct_batch = cot_Qwen(
                batch_q,
                "Let's Think Step By Step", 
                path_num, 
                batch_size,
                enable_verifier
            )

            total += total_batch
            correct += correct_batch
            print(f"âœ… æ‰¹æ¬¡å®Œæˆ | æœ¬æ‰¹æ­£ç¡®: {correct_batch}/{total_batch} | ç´¯è®¡æ­£ç¡®: {correct}/{total}")
        except KeyError as e:
            print(f"âŒ æ•°æ®å¼‚å¸¸: ç¼ºå°‘å…³é”®å­—æ®µ {e}")
        except Exception as e:
            print(f"âŒ å¤„ç†å¼‚å¸¸: {e}")
    
    if total > 0:
        accuracy = correct / total
        print(f"\n{'='*60}\nğŸ è¯„ä¼°å®Œæˆ | æ€»æ­£ç¡®: {correct}/{total} | å‡†ç¡®ç‡: {accuracy:.4f}\n{'='*60}")
        return accuracy
    else:
        print("âš ï¸ è¯„ä¼°å¤±è´¥ - æ— æœ‰æ•ˆé—®é¢˜å¤„ç†")
        return 0

# ç¤ºä¾‹ä½¿ç”¨æ–¹å¼
if __name__ == "__main__":
    # è¿è¡Œè¯„ä¼°ï¼ˆå¯ç”¨Verifierï¼‰
    evaluate_json_questions_with_api(
        json_path='AQuA/test.json',
        max_samples=10,
        batch_size=3,
        path_num=3,
        enable_verifier=True
    )
    
    # # è¿è¡Œè¯„ä¼°ï¼ˆç¦ç”¨Verifierï¼‰
    # evaluate_json_questions_with_api(
    #     json_path='AQuA/test.json',
    #     max_samples=10,
    #     batch_size=3,
    #     path_num=3,
    #     enable_verifier=False
    # )