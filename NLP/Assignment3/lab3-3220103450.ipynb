{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd3278a",
   "metadata": {},
   "source": [
    "<a id=\"part1\"></a>\n",
    "## 1. 环境配置与依赖安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d473eebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pip.modelarts.private.com:8888/repository/pypi/simple\n",
      "Requirement already satisfied: mindnlp==0.4.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (0.4.0)\n",
      "Requirement already satisfied: pillow>=10.0.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.4.0) (10.3.0)\n",
      "Requirement already satisfied: addict in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.4.0) (2.4.0)\n",
      "Requirement already satisfied: mindspore>=2.2.14 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.4.0) (2.4.1)\n",
      "Requirement already satisfied: tokenizers==0.19.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.4.0) (0.19.1)\n",
      "Requirement already satisfied: regex in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.4.0) (2024.11.6)\n",
      "Requirement already satisfied: pytest==7.2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.4.0) (7.2.0)\n",
      "Requirement already satisfied: evaluate in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.4.0) (0.4.3)\n",
      "Requirement already satisfied: pyctcdecode in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.4.0) (0.5.0)\n",
      "Requirement already satisfied: safetensors in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.4.0) (0.4.5)\n",
      "Requirement already satisfied: jieba in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.4.0) (0.42.1)\n",
      "Requirement already satisfied: ml-dtypes in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.4.0) (0.5.0)\n",
      "Requirement already satisfied: sentencepiece in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.4.0) (0.2.0)\n",
      "Requirement already satisfied: requests in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.4.0) (2.32.2)\n",
      "Requirement already satisfied: tqdm in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.4.0) (4.66.1)\n",
      "Requirement already satisfied: datasets in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.4.0) (2.18.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from pytest==7.2.0->mindnlp==0.4.0) (23.2.0)\n",
      "Requirement already satisfied: packaging in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from pytest==7.2.0->mindnlp==0.4.0) (24.1)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from pytest==7.2.0->mindnlp==0.4.0) (2.0.2)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from pytest==7.2.0->mindnlp==0.4.0) (1.5.0)\n",
      "Requirement already satisfied: iniconfig in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from pytest==7.2.0->mindnlp==0.4.0) (2.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from pytest==7.2.0->mindnlp==0.4.0) (1.2.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from tokenizers==0.19.1->mindnlp==0.4.0) (0.26.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1->mindnlp==0.4.0) (6.0.2)\n",
      "Requirement already satisfied: filelock in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1->mindnlp==0.4.0) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1->mindnlp==0.4.0) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1->mindnlp==0.4.0) (4.12.2)\n",
      "Requirement already satisfied: asttokens>=2.0.4 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore>=2.2.14->mindnlp==0.4.0) (2.4.1)\n",
      "Requirement already satisfied: protobuf>=3.13.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore>=2.2.14->mindnlp==0.4.0) (3.20.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.20.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore>=2.2.14->mindnlp==0.4.0) (1.23.0)\n",
      "Requirement already satisfied: scipy>=1.5.4 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore>=2.2.14->mindnlp==0.4.0) (1.12.0)\n",
      "Requirement already satisfied: psutil>=5.6.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore>=2.2.14->mindnlp==0.4.0) (6.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore>=2.2.14->mindnlp==0.4.0) (1.6.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from asttokens>=2.0.4->mindspore>=2.2.14->mindnlp==0.4.0) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from astunparse>=1.6.3->mindspore>=2.2.14->mindnlp==0.4.0) (0.38.4)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from datasets->mindnlp==0.4.0) (0.6)\n",
      "Requirement already satisfied: xxhash in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from datasets->mindnlp==0.4.0) (3.5.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from datasets->mindnlp==0.4.0) (0.3.8)\n",
      "Requirement already satisfied: multiprocess in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from datasets->mindnlp==0.4.0) (0.70.16)\n",
      "Requirement already satisfied: pandas in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from datasets->mindnlp==0.4.0) (1.3.5)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from datasets->mindnlp==0.4.0) (12.0.1)\n",
      "Requirement already satisfied: aiohttp in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from datasets->mindnlp==0.4.0) (3.10.10)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.4.0) (1.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.4.0) (2.4.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.4.0) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.4.0) (1.17.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.4.0) (6.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.4.0) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests->mindnlp==0.4.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests->mindnlp==0.4.0) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests->mindnlp==0.4.0) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests->mindnlp==0.4.0) (1.26.7)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->mindnlp==0.4.0) (0.2.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from pandas->datasets->mindnlp==0.4.0) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from pandas->datasets->mindnlp==0.4.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pygtrie<3.0,>=2.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from pyctcdecode->mindnlp==0.4.0) (2.5.0)\n",
      "Requirement already satisfied: hypothesis<7,>=6.14 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from pyctcdecode->mindnlp==0.4.0) (6.131.17)\n",
      "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from hypothesis<7,>=6.14->pyctcdecode->mindnlp==0.4.0) (2.4.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/MindSpore/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: Skipping mindformers as it is not installed.\u001b[0m\n",
      "Found existing installation: transformers 4.40.0\n",
      "Uninstalling transformers-4.40.0:\n",
      "  Successfully uninstalled transformers-4.40.0\n",
      "Looking in indexes: http://pip.modelarts.private.com:8888/repository/pypi/simple\n",
      "Collecting transformers==4.40.0\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/transformers/4.40.0/transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.0 MB 52.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: safetensors>=0.4.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from transformers==4.40.0) (0.4.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from transformers==4.40.0) (6.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from transformers==4.40.0) (24.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from transformers==4.40.0) (4.66.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from transformers==4.40.0) (0.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from transformers==4.40.0) (0.26.2)\n",
      "Requirement already satisfied: requests in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from transformers==4.40.0) (2.32.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from transformers==4.40.0) (1.23.0)\n",
      "Requirement already satisfied: filelock in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from transformers==4.40.0) (3.16.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from transformers==4.40.0) (2024.11.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (2024.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests->transformers==4.40.0) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests->transformers==4.40.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests->transformers==4.40.0) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests->transformers==4.40.0) (3.10)\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-4.40.0\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/MindSpore/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: http://pip.modelarts.private.com:8888/repository/pypi/simple\n",
      "Requirement already satisfied: mindspore==2.4.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (2.4.1)\n",
      "Requirement already satisfied: scipy>=1.5.4 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore==2.4.1) (1.12.0)\n",
      "Requirement already satisfied: safetensors>=0.4.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore==2.4.1) (0.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore==2.4.1) (10.3.0)\n",
      "Requirement already satisfied: asttokens>=2.0.4 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore==2.4.1) (2.4.1)\n",
      "Requirement already satisfied: astunparse>=1.6.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore==2.4.1) (1.6.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.20.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore==2.4.1) (1.23.0)\n",
      "Requirement already satisfied: protobuf>=3.13.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore==2.4.1) (3.20.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore==2.4.1) (24.1)\n",
      "Requirement already satisfied: psutil>=5.6.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore==2.4.1) (6.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from asttokens>=2.0.4->mindspore==2.4.1) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from astunparse>=1.6.3->mindspore==2.4.1) (0.38.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/MindSpore/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "env: HF_ENDPOINT=https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "!pip install mindnlp==0.4.0\n",
    "!pip uninstall mindformers transformers -y\n",
    "!pip install transformers==4.40.0\n",
    "!pip install mindspore==2.4.1\n",
    "%env HF_ENDPOINT=https://hf-mirror.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e64786",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "## 2. 数据预处理扩展（情感分类任务）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a1e74c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(3195:281473158549680,MainProcess):2025-05-17-13:00:37.617.526 [mindspore/run_check/_check_version.py:398] Can not find the tbe operator implementation(need by mindspore-ascend). Please check whether the Environment Variable PYTHONPATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] ME(3195:281473158549680,MainProcess):2025-05-17-13:00:37.620.126 [mindspore/run_check/_check_version.py:398] Can not find the tbe operator implementation(need by mindspore-ascend). Please check whether the Environment Variable PYTHONPATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import codecs\n",
    "from pathlib import Path\n",
    "\n",
    "import mindspore\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor\n",
    "from mindspore import context\n",
    "from mindspore.train.model import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "from mindspore.ops import operations as ops\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import mindspore\n",
    "from mindnlp.core.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "from mindnlp.dataset import load_dataset\n",
    "from mindnlp.engine import set_seed\n",
    "from mindnlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from mindnlp.transformers.optimization import get_linear_schedule_with_warmup\n",
    "\n",
    "from mindnlp.peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    PeftType,\n",
    "    PromptTuningConfig,\n",
    ")\n",
    "\n",
    "mindspore.set_context(device_target='Ascend')\n",
    "\n",
    "class MovieReview:\n",
    "    \"\"\"情感分类数据预处理（移植自TextCNN_MindSpore.ipynb）\"\"\"\n",
    "    def __init__(self, root_dir, maxlen=51, split=0.0):\n",
    "        self.path = root_dir\n",
    "        self.feelMap = {'neg':0, 'pos':1}\n",
    "        self.files = []\n",
    "        \n",
    "        self.Vocab = {}  # 显式创建词表字典\n",
    "        \n",
    "        mypath = Path(self.path)\n",
    "        if not mypath.exists() or not mypath.is_dir():\n",
    "            print(\"please check the root_dir!\")\n",
    "            raise ValueError\n",
    "\n",
    "        # 在数据目录中找到文件\n",
    "        for root,_,filename in os.walk(self.path):\n",
    "            for each in filename:\n",
    "                self.files.append(os.path.join(root,each))\n",
    "            break\n",
    "\n",
    "        # 确认是否为两个文件.neg与.pos\n",
    "        if len(self.files) != 2:\n",
    "            print(\"There are {} files in the root_dir\".format(len(self.files)))\n",
    "            raise ValueError\n",
    "\n",
    "        # 读取数据\n",
    "        self.word_num = 0\n",
    "        self.maxlen = 0\n",
    "        self.minlen = float(\"inf\")\n",
    "        self.maxlen = float(\"-inf\")\n",
    "        self.Pos = []\n",
    "        self.Neg = []\n",
    "        for filename in self.files:\n",
    "            f = codecs.open(filename, 'r')\n",
    "            ff = f.read()\n",
    "            file_object = codecs.open(filename, 'w', 'utf-8')\n",
    "            file_object.write(ff)\n",
    "            self.read_data(filename)\n",
    "        self.PosNeg = self.Pos + self.Neg\n",
    "\n",
    "        self.text2vec(maxlen=maxlen)\n",
    "        self.split_dataset(split=split)\n",
    "\n",
    "    def read_data(self, filePath):\n",
    "\n",
    "        with open(filePath,'r') as f:\n",
    "            \n",
    "            for sentence in f.readlines():\n",
    "                sentence = sentence.replace('\\n','')\\\n",
    "                                    .replace('\"','')\\\n",
    "                                    .replace('\\'','')\\\n",
    "                                    .replace('.','')\\\n",
    "                                    .replace(',','')\\\n",
    "                                    .replace('[','')\\\n",
    "                                    .replace(']','')\\\n",
    "                                    .replace('(','')\\\n",
    "                                    .replace(')','')\\\n",
    "                                    .replace(':','')\\\n",
    "                                    .replace('--','')\\\n",
    "                                    .replace('-',' ')\\\n",
    "                                    .replace('\\\\','')\\\n",
    "                                    .replace('0','')\\\n",
    "                                    .replace('1','')\\\n",
    "                                    .replace('2','')\\\n",
    "                                    .replace('3','')\\\n",
    "                                    .replace('4','')\\\n",
    "                                    .replace('5','')\\\n",
    "                                    .replace('6','')\\\n",
    "                                    .replace('7','')\\\n",
    "                                    .replace('8','')\\\n",
    "                                    .replace('9','')\\\n",
    "                                    .replace('`','')\\\n",
    "                                    .replace('=','')\\\n",
    "                                    .replace('$','')\\\n",
    "                                    .replace('/','')\\\n",
    "                                    .replace('*','')\\\n",
    "                                    .replace(';','')\\\n",
    "                                    .replace('<b>','')\\\n",
    "                                    .replace('%','')\n",
    "                sentence = sentence.split(' ')\n",
    "                sentence = list(filter(lambda x: x, sentence))\n",
    "                if sentence:\n",
    "                    self.word_num += len(sentence)\n",
    "                    self.maxlen = self.maxlen if self.maxlen >= len(sentence) else len(sentence)\n",
    "                    self.minlen = self.minlen if self.minlen <= len(sentence) else len(sentence)\n",
    "                    if 'pos' in filePath:\n",
    "                        self.Pos.append([sentence,self.feelMap['pos']])\n",
    "                    else:\n",
    "                        self.Neg.append([sentence,self.feelMap['neg']])\n",
    "        \n",
    "    def text2vec(self, maxlen):\n",
    "        \"\"\"文本向量化\"\"\"\n",
    "        for sentence_label in self.Pos + self.Neg:\n",
    "            vector = [0]*maxlen\n",
    "            for idx, word in enumerate(sentence_label[0]):\n",
    "                if idx >= maxlen: break\n",
    "                if word not in self.Vocab:\n",
    "                    self.Vocab[word] = len(self.Vocab)\n",
    "                vector[idx] = self.Vocab[word]\n",
    "            sentence_label[0] = vector\n",
    "    \n",
    "    def create_dataset(self, batch_size=64):\n",
    "        \"\"\"生成MindSpore Dataset\"\"\"\n",
    "        def generator():\n",
    "            for item in self.train:\n",
    "                yield (np.array(item[0], dtype=np.int32), \n",
    "                       np.array(item[1], dtype=np.int32))\n",
    "        return ds.GeneratorDataset(source=generator(), \n",
    "                              column_names=[\"input_ids\", \"labels\"]).batch(batch_size)\n",
    "    def split_dataset(self, split=0.0):  # split参数无效\n",
    "        self.train = self.Pos + self.Neg\n",
    "        random.shuffle(self.train)\n",
    "        self.test = self.train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea2b1ce",
   "metadata": {},
   "source": [
    "<a id=\"part3\"></a>\n",
    "## 3. 硬提示推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3098d4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "硬提示直接推理（没经过训练）\n",
      "提示模板: '这个句子的情感是：'\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at AI-ModelScope/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "硬提示推理结果:\n",
      "- 总样本数: 10688\n",
      "- 准确率: 50.00%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def inference_with_hard_prompt():\n",
    "    # --- 配置 ---\n",
    "    HARD_PROMPT = \"这个句子的情感是：\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"硬提示直接推理（没经过训练）\")\n",
    "    print(f\"提示模板: '{HARD_PROMPT}'\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # --- 加载模型 ---\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"AI-ModelScope/roberta-large\", \n",
    "        num_labels=2  # 二分类任务\n",
    "    )\n",
    "    \n",
    "    # --- 加载数据并添加提示 ---\n",
    "    sentiment_data = MovieReview(root_dir=\"./data/\", maxlen=51)\n",
    "    vocab_inv = {v:k for k,v in sentiment_data.Vocab.items()}  # 反向词表\n",
    "    dataset = sentiment_data.create_dataset(batch_size=32)\n",
    "    \n",
    "    # --- 推理与评估 ---\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    model.set_train(False)  # 设置为推理模式\n",
    "    \n",
    "    for batch in dataset.create_dict_iterator():\n",
    "        # 将词索引转换为文本并添加提示\n",
    "        batch_texts = [\n",
    "            f\"{HARD_PROMPT}{' '.join([vocab_inv[idx] for idx in seq if idx != 0])}\" \n",
    "            for seq in batch[\"input_ids\"].asnumpy()\n",
    "        ]\n",
    "        \n",
    "        # 编码输入\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=\"max_length\",    # 自动填充到最大长度\n",
    "            truncation=True,         # 启用截断\n",
    "            max_length=64,           # 设置与训练一致的max_length\n",
    "            return_tensors=\"ms\"\n",
    "        )[\"input_ids\"]\n",
    "        \n",
    "        # 推理\n",
    "        outputs = model(inputs)\n",
    "        preds = outputs.logits.argmax(-1).asnumpy()\n",
    "        \n",
    "        # 计算指标\n",
    "        metric.add_batch(\n",
    "            predictions=preds,\n",
    "            references=batch[\"labels\"].asnumpy()\n",
    "        )\n",
    "    \n",
    "    # 输出结果\n",
    "    acc = metric.compute()[\"accuracy\"]\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"硬提示推理结果:\")\n",
    "    print(f\"- 总样本数: {len(dataset)*32}\")\n",
    "    print(f\"- 准确率: {acc*100:.2f}%\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# ===== 执行推理 =====\n",
    "if __name__ == \"__main__\":\n",
    "    inference_with_hard_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b42ae72-5e3a-474e-baf3-fecd84b1d53a",
   "metadata": {},
   "source": [
    "<a id=\"part4\"></a>\n",
    "## 4. 软提示训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1a82070a-d279-4404-852f-fc33d18315f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 软提示配置 ---\n",
    "batch_size = 8\n",
    "model_name_or_path = \"AI-ModelScope/roberta-large\"\n",
    "task = \"mrpc\"\n",
    "peft_type = PeftType.PROMPT_TUNING\n",
    "# num_epochs = 20\n",
    "num_epochs = 5\n",
    "\n",
    "# peft config\n",
    "peft_config = PromptTuningConfig(task_type=\"SEQ_CLS\", num_virtual_tokens=10)\n",
    "# learning rate\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c260657b-4bc0-4653-9ff3-acfa0885dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "if any(k in model_name_or_path for k in (\"gpt\", \"opt\", \"bloom\")):\n",
    "    padding_side = \"left\"\n",
    "else:\n",
    "    padding_side = \"right\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=padding_side, mirror=\"modelscope\")\n",
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f36c5700-224f-49cc-a50b-6ee4a343d16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': Tensor(shape=[], dtype=String, value= 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .'), 'sentence2': Tensor(shape=[], dtype=String, value= 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .'), 'label': Tensor(shape=[], dtype=Int64, value= 1), 'idx': Tensor(shape=[], dtype=Int64, value= 0)}\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(\"glue\", task)\n",
    "print(next(datasets['train'].create_dict_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9fb25245-ff56-4e0c-941f-7f106102b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.dataset import BaseMapFunction\n",
    "\n",
    "class MapFunc(BaseMapFunction):\n",
    "    def __call__(self, sentence1, sentence2, label, idx):\n",
    "        outputs = tokenizer(sentence1, sentence2, truncation=True, max_length=None)\n",
    "        return outputs['input_ids'], outputs['attention_mask'], label\n",
    "\n",
    "\n",
    "def get_dataset(dataset, tokenizer):\n",
    "    input_colums=['sentence1', 'sentence2', 'label', 'idx']\n",
    "    output_columns=['input_ids', 'attention_mask', 'labels']\n",
    "    dataset = dataset.map(MapFunc(input_colums, output_columns),\n",
    "                          input_colums, output_columns)\n",
    "    dataset = dataset.padded_batch(batch_size, pad_info={'input_ids': (None, tokenizer.pad_token_id),\n",
    "                                                         'attention_mask': (None, 0)})\n",
    "    return dataset\n",
    "\n",
    "train_dataset = get_dataset(datasets['train'], tokenizer)\n",
    "eval_dataset = get_dataset(datasets['validation'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "274825cf-a0b2-4397-92dd-95c66c12ae1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': Tensor(shape=[8, 67], dtype=Int64, value=\n",
      "[[    0, 10127,  1001 ...     1,     1,     1],\n",
      " [    0,   975, 26802 ...     1,     1,     1],\n",
      " [    0,  1213,    56 ...     1,     1,     1],\n",
      " ...\n",
      " [    0,  9064, 32497 ...     1,     1,     1],\n",
      " [    0,   133,  4417 ...     1,     1,     1],\n",
      " [    0,   133, 19888 ...     1,     1,     1]]), 'attention_mask': Tensor(shape=[8, 67], dtype=Int64, value=\n",
      "[[1, 1, 1 ... 0, 0, 0],\n",
      " [1, 1, 1 ... 0, 0, 0],\n",
      " [1, 1, 1 ... 0, 0, 0],\n",
      " ...\n",
      " [1, 1, 1 ... 0, 0, 0],\n",
      " [1, 1, 1 ... 0, 0, 0],\n",
      " [1, 1, 1 ... 0, 0, 0]]), 'labels': Tensor(shape=[8], dtype=Int64, value= [1, 0, 1, 0, 1, 1, 0, 1])}\n"
     ]
    }
   ],
   "source": [
    "print(next(train_dataset.create_dict_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6472bea0-c694-4edb-9e19-80098847b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"glue\", task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5c6a268b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at AI-ModelScope/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,061,890 || all params: 356,423,684 || trainable%: 0.2979291353713745\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, return_dict=True, mirror=\"modelscope\")\n",
    "model = get_peft_model(model, peft_config)\n",
    "# print number of trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2004be87",
   "metadata": {},
   "source": [
    "模型微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "79f9642a-6328-47d6-946f-9b6259e35367",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(params=model.trainable_params(), lr=lr)\n",
    "\n",
    "# Instantiate scheduler\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0.06 * (len(train_dataset) * num_epochs),\n",
    "    num_training_steps=(len(train_dataset) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e5f94b31-3bc7-4296-8df2-6e284620472b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(shape=[1024, 1024], dtype=Float32, value=\n",
       " [[ 1.38089573e-02,  2.89921463e-02,  1.32645434e-02 ... -2.05632485e-02, -2.86122505e-03,  2.10555382e-02],\n",
       "  [ 4.60442342e-03,  4.34034653e-02, -8.66422988e-03 ... -2.42341571e-02,  1.46407923e-02, -2.72690002e-02],\n",
       "  [ 1.18833138e-02, -6.02551457e-03, -1.69392396e-02 ... -4.50211251e-03,  7.50867603e-03,  3.92194232e-03],\n",
       "  ...\n",
       "  [ 3.13470233e-03, -4.23548520e-02,  2.29569934e-02 ... -2.14548297e-02, -2.52000801e-02, -1.21975020e-02],\n",
       "  [ 2.33502574e-02,  1.67301262e-03, -9.02926270e-03 ... -2.08490994e-02, -2.75446083e-02, -8.77535250e-03],\n",
       "  [ 2.20639468e-03, -2.47005536e-03,  5.73945278e-03 ... -2.47196835e-02, -3.58031616e-02, -9.51369666e-03]]),\n",
       " Tensor(shape=[1024], dtype=Float32, value= [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00]),\n",
       " Tensor(shape=[2, 1024], dtype=Float32, value=\n",
       " [[-3.27868313e-02, -2.51472760e-02, -3.97190116e-02 ... -1.66799501e-02, -8.69967137e-03, -2.72309836e-02],\n",
       "  [-4.85193171e-03, -2.03677900e-02, -1.18026659e-02 ...  6.50600204e-03, -1.30546524e-03,  5.37372427e-03]]),\n",
       " Tensor(shape=[2], dtype=Float32, value= [ 0.00000000e+00,  0.00000000e+00]),\n",
       " Tensor(shape=[10, 1024], dtype=Float32, value=\n",
       " [[-2.25794172e+00, -1.06826377e+00, -1.03289509e+00 ...  2.80281991e-01, -9.47517514e-01, -1.26439142e+00],\n",
       "  [-1.61189567e-02, -1.07946306e-01,  5.52362680e-01 ... -1.23875117e+00,  3.42920125e-01,  7.67217398e-01],\n",
       "  [-5.13667822e-01,  3.27602446e-01,  9.58116293e-01 ...  7.82477204e-03,  4.81719464e-01, -6.11795843e-01],\n",
       "  ...\n",
       "  [-6.00380421e-01,  1.73791811e-01,  3.90712678e-01 ... -2.07375050e+00,  1.40761602e+00, -4.37822402e-01],\n",
       "  [-7.59839594e-01, -1.78946459e+00,  4.80047345e-01 ...  7.57634878e-01,  9.20863569e-01,  1.26784480e+00],\n",
       "  [-6.21206522e-01,  6.04873717e-01,  1.06483400e+00 ...  1.11680657e-01,  2.19889307e+00,  1.17284584e+00]]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print name of trainable parameters\n",
    "model.trainable_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2247fce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/459 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/459 [00:13<1:39:17, 13.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 458/459 [02:06<00:00,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 459/459 [02:17<00:00,  3.33it/s]\n",
      "100%|██████████| 51/51 [00:03<00:00, 14.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 459/459 [01:16<00:00,  5.99it/s]\n",
      "100%|██████████| 51/51 [00:03<00:00, 14.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: {'accuracy': 0.6887254901960784, 'f1': 0.8140556368960469}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 459/459 [01:18<00:00,  5.87it/s]\n",
      "100%|██████████| 51/51 [00:03<00:00, 14.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: {'accuracy': 0.7132352941176471, 'f1': 0.8208269525267994}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 459/459 [01:33<00:00,  4.92it/s]\n",
      "100%|██████████| 51/51 [00:04<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: {'accuracy': 0.7083333333333334, 'f1': 0.7965811965811965}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 459/459 [01:29<00:00,  5.14it/s]\n",
      "100%|██████████| 51/51 [00:03<00:00, 13.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: {'accuracy': 0.7034313725490197, 'f1': 0.8169440242057489}\n"
     ]
    }
   ],
   "source": [
    "def forward_fn(**batch):\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    return loss\n",
    "\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, model.trainable_params())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.set_train()\n",
    "    train_total_size = train_dataset.get_dataset_size()\n",
    "    for step, batch in enumerate(tqdm(train_dataset.create_dict_iterator(), total=train_total_size)):\n",
    "\n",
    "        loss, grads = grad_fn(**batch)\n",
    "        optimizer.step(grads)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    model.set_train(False)\n",
    "    eval_total_size = eval_dataset.get_dataset_size()\n",
    "    for step, batch in enumerate(tqdm(eval_dataset.create_dict_iterator(), total=eval_total_size)):\n",
    "        outputs = model(**batch)\n",
    "        predictions = outputs.logits.argmax(axis=-1)\n",
    "        predictions, references = predictions, batch[\"labels\"]\n",
    "        metric.add_batch(\n",
    "            predictions=predictions,\n",
    "            references=references,\n",
    "        )\n",
    "\n",
    "    eval_metric = metric.compute()\n",
    "    print(f\"epoch {epoch}:\", eval_metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
