{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MindSpore-Transformer-Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 导入依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "import mindspore.nn as nn\n",
    "from mindspore import context\n",
    "import mindspore.dataset.engine as de\n",
    "import mindspore.common.dtype as mstype\n",
    "from mindspore.mindrecord import FileWriter\n",
    "from mindspore.common.parameter import Parameter\n",
    "import mindspore.dataset.transforms.c_transforms as deC\n",
    "from mindspore.common.tensor import Tensor\n",
    "from mindspore.nn.optim import Adam\n",
    "from mindspore.train.model import Model\n",
    "from mindspore.train.loss_scale_manager import DynamicLossScaleManager\n",
    "from mindspore.train.callback import CheckpointConfig, ModelCheckpoint\n",
    "from mindspore.train.callback import Callback, TimeMonitor\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "\n",
    "from src import tokenization\n",
    "from src.train_util import LossCallBack\n",
    "from src.lr_schedule import create_dynamic_lr\n",
    "from src.transformer_model import TransformerConfig, TransformerModel\n",
    "from src.data_utils import create_training_instance, write_instance_to_file\n",
    "from src.transformer_for_train import TransformerTrainOneStepCell, TransformerNetworkWithLoss, TransformerTrainOneStepWithLossScaleCell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 设置运行环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"Ascend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 定义数据处理相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_cfg = edict({\n",
    "        'input_file': './data/ch_en_all.txt',\n",
    "        'vocab_file': './data/ch_en_vocab.txt',\n",
    "        'train_file_mindrecord': './data/train.mindrecord',\n",
    "        'eval_file_mindrecord': './data/test.mindrecord',\n",
    "        'train_file_source': './data/source_train.txt',\n",
    "        'eval_file_source': './data/source_test.txt',\n",
    "        'num_splits':1,\n",
    "        'clip_to_max_len': False,\n",
    "        'max_seq_length': 40\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 定义数据处理函数\n",
    "\n",
    "加载原始数据，切分训练、测试数据，并预处理成模型输入所需的数据形式，并保存为mindrecord格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def data_prepare(cfg, eval_idx):\n",
    "    tokenizer = tokenization.WhiteSpaceTokenizer(vocab_file=cfg.vocab_file)\n",
    "\n",
    "    writer_train = FileWriter(cfg.train_file_mindrecord, cfg.num_splits)\n",
    "    writer_eval = FileWriter(cfg.eval_file_mindrecord, cfg.num_splits)\n",
    "    data_schema = {\"source_sos_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "                   \"source_sos_mask\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "                   \"source_eos_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "                   \"source_eos_mask\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "                   \"target_sos_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "                   \"target_sos_mask\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "                   \"target_eos_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "                   \"target_eos_mask\": {\"type\": \"int32\", \"shape\": [-1]}\n",
    "                   }\n",
    "\n",
    "    writer_train.add_schema(data_schema, \"tranformer train\")\n",
    "    writer_eval.add_schema(data_schema, \"tranformer eval\")\n",
    "\n",
    "    index = 0\n",
    "    f_train = open(cfg.train_file_source, 'w', encoding='utf-8')\n",
    "    f_test = open(cfg.eval_file_source,'w',encoding='utf-8')\n",
    "    f = open(cfg.input_file, \"r\", encoding='utf-8')\n",
    "    for s_line in f:\n",
    "        print(\"finish {}/{}\".format(index, 23607), end='\\r')\n",
    "        \n",
    "        line = tokenization.convert_to_unicode(s_line)\n",
    "\n",
    "        source_line, target_line = line.strip().split(\"\\t\")\n",
    "        source_tokens = tokenizer.tokenize(source_line)\n",
    "        target_tokens = tokenizer.tokenize(target_line)\n",
    "\n",
    "        if len(source_tokens) >= (cfg.max_seq_length-1) or len(target_tokens) >= (cfg.max_seq_length-1):\n",
    "            if cfg.clip_to_max_len:\n",
    "                source_tokens = source_tokens[:cfg.max_seq_length-1]\n",
    "                target_tokens = target_tokens[:cfg.max_seq_length-1]\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        index = index + 1\n",
    "        # print(source_tokens)\n",
    "        instance = create_training_instance(source_tokens, target_tokens, cfg.max_seq_length)\n",
    "        \n",
    "        if index in eval_idx:\n",
    "            f_test.write(s_line)\n",
    "            features = write_instance_to_file(writer_eval, instance, tokenizer, cfg.max_seq_length)\n",
    "        else:\n",
    "            f_train.write(s_line)\n",
    "            features = write_instance_to_file(writer_train, instance, tokenizer, cfg.max_seq_length)\n",
    "    f.close()\n",
    "    f_test.close()\n",
    "    f_train.close()\n",
    "    writer_train.commit()\n",
    "    writer_eval.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 数据处理，随机选20%作为测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 23607/23607\r"
     ]
    }
   ],
   "source": [
    "sample_num = 23607\n",
    "eval_idx = np.random.choice(sample_num, int(sample_num*0.2), replace=False)\n",
    "data_prepare(data_cfg, eval_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 定义数据加载函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(batch_size=1, data_file=None):\n",
    "    \"\"\"\n",
    "    Load mindrecord dataset\n",
    "    \"\"\"\n",
    "    ds = de.MindDataset(data_file,\n",
    "                        columns_list=[\"source_eos_ids\", \"source_eos_mask\",\n",
    "                                      \"target_sos_ids\", \"target_sos_mask\",\n",
    "                                      \"target_eos_ids\", \"target_eos_mask\"],\n",
    "                        shuffle=False)\n",
    "    type_cast_op = deC.TypeCast(mstype.int32)\n",
    "    ds = ds.map(input_columns=\"source_eos_ids\", operations=type_cast_op)\n",
    "    ds = ds.map(input_columns=\"source_eos_mask\", operations=type_cast_op)\n",
    "    ds = ds.map(input_columns=\"target_sos_ids\", operations=type_cast_op)\n",
    "    ds = ds.map(input_columns=\"target_sos_mask\", operations=type_cast_op)\n",
    "    ds = ds.map(input_columns=\"target_eos_ids\", operations=type_cast_op)\n",
    "    ds = ds.map(input_columns=\"target_eos_mask\", operations=type_cast_op)\n",
    "    # apply batch operations\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    ds.channel_name = 'transformer'\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试数据是否能正常加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "next(load_dataset(data_file=data_cfg.train_file_mindrecord).create_dict_iterator())['source_eos_ids'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 定义训练相关配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_cfg = edict({\n",
    "    #--------------------------------------nework confige-------------------------------------\n",
    "    'transformer_network': 'base',\n",
    "    'init_loss_scale_value': 1024,\n",
    "    'scale_factor': 2,\n",
    "    'scale_window': 2000,\n",
    "\n",
    "    'lr_schedule': edict({\n",
    "        'learning_rate': 1.0,\n",
    "        'warmup_steps': 8000,\n",
    "        'start_decay_step': 16000,\n",
    "        'min_lr': 0.0,\n",
    "    }),\n",
    "    #-----------------------------------save model confige-------------------------\n",
    "    'enable_save_ckpt': True ,        #Enable save checkpointdefault is true.\n",
    "    'save_checkpoint_steps':590,   #Save checkpoint steps, default is 590.\n",
    "    'save_checkpoint_num':2,     #Save checkpoint numbers, default is 2.\n",
    "    'save_checkpoint_path': './checkpoint',    #Save checkpoint file path,default is ./checkpoint/\n",
    "    'save_checkpoint_name':'transformer-32_40',\n",
    "    'checkpoint_path':'',     #Checkpoint file path\n",
    "    \n",
    "    \n",
    "    #-------------------------------device confige-----------------------------\n",
    "    'enable_data_sink':False,   #Enable data sink, default is False.\n",
    "    'device_id':0,\n",
    "    'device_num':1,\n",
    "    'distribute':False,\n",
    "    \n",
    "    # -----------------mast same with the dataset-----------------------\n",
    "    'seq_length':40,\n",
    "    'vocab_size':10067,\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    'data_path':\"./data/train.mindrecord\",   #Data path\n",
    "    'epoch_size':15,\n",
    "    'batch_size':32,\n",
    "    'max_position_embeddings':40,\n",
    "    'enable_lossscale': False,       #Use lossscale or not, default is False.\n",
    "    'do_shuffle':True       #Enable shuffle for dataset, default is True.\n",
    "})\n",
    "'''\n",
    "two kinds of transformer model version\n",
    "'''\n",
    "if train_cfg.transformer_network == 'base':\n",
    "    transformer_net_cfg = TransformerConfig(\n",
    "        batch_size=train_cfg.batch_size,\n",
    "        seq_length=train_cfg.seq_length,\n",
    "        vocab_size=train_cfg.vocab_size,\n",
    "        hidden_size=512,\n",
    "        num_hidden_layers=6,\n",
    "        num_attention_heads=8,\n",
    "        intermediate_size=2048,\n",
    "        hidden_act=\"relu\",\n",
    "        hidden_dropout_prob=0.2,\n",
    "        attention_probs_dropout_prob=0.2,\n",
    "        max_position_embeddings=train_cfg.max_position_embeddings,\n",
    "        initializer_range=0.02,\n",
    "        label_smoothing=0.1,\n",
    "        input_mask_from_dataset=True,\n",
    "        dtype=mstype.float32,\n",
    "        compute_type=mstype.float16)\n",
    "elif train_cfg.transformer_network == 'large':\n",
    "    transformer_net_cfg = TransformerConfig(\n",
    "        batch_size=train_cfg.batch_size,\n",
    "        seq_length=train_cfg.seq_length,\n",
    "        vocab_size=train_cfg.vocab_size,\n",
    "        hidden_size=1024,\n",
    "        num_hidden_layers=6,\n",
    "        num_attention_heads=16,\n",
    "        intermediate_size=4096,\n",
    "        hidden_act=\"relu\",\n",
    "        hidden_dropout_prob=0.2,\n",
    "        attention_probs_dropout_prob=0.2,\n",
    "        max_position_embeddings=train_cfg.max_position_embeddings,\n",
    "        initializer_range=0.02,\n",
    "        label_smoothing=0.1,\n",
    "        input_mask_from_dataset=True,\n",
    "        dtype=mstype.float32,\n",
    "        compute_type=mstype.float16)\n",
    "else:\n",
    "    raise Exception(\"The src/train_confige of transformer_network must base or large. Change the str/train_confige file and try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(cfg):\n",
    "    \"\"\"\n",
    "    Transformer training.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_dataset = load_dataset(cfg.batch_size, data_file=cfg.data_path)\n",
    "\n",
    "    netwithloss = TransformerNetworkWithLoss(transformer_net_cfg, True)\n",
    "\n",
    "    if cfg.checkpoint_path:\n",
    "        parameter_dict = load_checkpoint(cfg.checkpoint_path)\n",
    "        load_param_into_net(netwithloss, parameter_dict)\n",
    "\n",
    "    lr = Tensor(create_dynamic_lr(schedule=\"constant*rsqrt_hidden*linear_warmup*rsqrt_decay\",\n",
    "                                  training_steps=train_dataset.get_dataset_size()*cfg.epoch_size,\n",
    "                                  learning_rate=cfg.lr_schedule.learning_rate,\n",
    "                                  warmup_steps=cfg.lr_schedule.warmup_steps,\n",
    "                                  hidden_size=transformer_net_cfg.hidden_size,\n",
    "                                  start_decay_step=cfg.lr_schedule.start_decay_step,\n",
    "                                  min_lr=cfg.lr_schedule.min_lr), mstype.float32)\n",
    "    optimizer = Adam(netwithloss.trainable_params(), lr)\n",
    "\n",
    "    callbacks = [TimeMonitor(train_dataset.get_dataset_size()), LossCallBack()]\n",
    "    if cfg.enable_save_ckpt:\n",
    "        ckpt_config = CheckpointConfig(save_checkpoint_steps=cfg.save_checkpoint_steps,\n",
    "                                       keep_checkpoint_max=cfg.save_checkpoint_num)\n",
    "        ckpoint_cb = ModelCheckpoint(prefix=cfg.save_checkpoint_name, directory=cfg.save_checkpoint_path, config=ckpt_config)\n",
    "        callbacks.append(ckpoint_cb)\n",
    "\n",
    "    if cfg.enable_lossscale:\n",
    "        scale_manager = DynamicLossScaleManager(init_loss_scale=cfg.init_loss_scale_value,\n",
    "                                                scale_factor=cfg.scale_factor,\n",
    "                                                scale_window=cfg.scale_window)\n",
    "        update_cell = scale_manager.get_update_cell()\n",
    "        netwithgrads = TransformerTrainOneStepWithLossScaleCell(netwithloss, optimizer=optimizer,scale_update_cell=update_cell)\n",
    "    else:\n",
    "        netwithgrads = TransformerTrainOneStepCell(netwithloss, optimizer=optimizer)\n",
    "\n",
    "    netwithgrads.set_train(True)\n",
    "    model = Model(netwithgrads)\n",
    "    model.train(cfg.epoch_size, train_dataset, callbacks=callbacks, dataset_sink_mode=cfg.enable_data_sink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. 启动训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train(train_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. 定义推理相关参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eval_cfg = edict({\n",
    "    'transformer_network': 'base',\n",
    "    \n",
    "    'data_file': './data/test.mindrecord',\n",
    "    'test_source_file':'./data/source_test.txt',\n",
    "    'model_file': './checkpoint/transformer-32_40-15_590.ckpt' ,\n",
    "    'vocab_file':'./data/ch_en_vocab.txt',\n",
    "    'token_file': './token-32-40.txt',\n",
    "    'pred_file':'./pred-32-40.txt',\n",
    "    \n",
    "    # -------------------mast same with the train config and the datsset------------------------\n",
    "    'seq_length':40,\n",
    "    'vocab_size':10067,\n",
    "\n",
    "    #-------------------------------------eval config-----------------------------\n",
    "    'batch_size':32,\n",
    "    'max_position_embeddings':40       # mast same with the train config\n",
    "})\n",
    "\n",
    "'''\n",
    "two kinds of transformer model version\n",
    "'''\n",
    "if eval_cfg.transformer_network == 'base':\n",
    "    transformer_net_cfg = TransformerConfig(\n",
    "        batch_size=eval_cfg.batch_size,\n",
    "        seq_length=eval_cfg.seq_length,\n",
    "        vocab_size=eval_cfg.vocab_size,\n",
    "        hidden_size=512,\n",
    "        num_hidden_layers=6,\n",
    "        num_attention_heads=8,\n",
    "        intermediate_size=2048,\n",
    "        hidden_act=\"relu\",\n",
    "        hidden_dropout_prob=0.0,\n",
    "        attention_probs_dropout_prob=0.0,\n",
    "        max_position_embeddings=eval_cfg.max_position_embeddings,\n",
    "        label_smoothing=0.1,\n",
    "        input_mask_from_dataset=True,\n",
    "        beam_width=4,\n",
    "        max_decode_length=eval_cfg.seq_length,\n",
    "        length_penalty_weight=1.0,\n",
    "        dtype=mstype.float32,\n",
    "        compute_type=mstype.float16)\n",
    "    \n",
    "elif eval_cfg.transformer_network == 'large':\n",
    "    transformer_net_cfg = TransformerConfig(\n",
    "        batch_size=eval_cfg.batch_size,\n",
    "        seq_length=eval_cfg.seq_length,\n",
    "        vocab_size=eval_cfg.vocab_size,\n",
    "        hidden_size=1024,\n",
    "        num_hidden_layers=6,\n",
    "        num_attention_heads=16,\n",
    "        intermediate_size=4096,\n",
    "        hidden_act=\"relu\",\n",
    "        hidden_dropout_prob=0.0,\n",
    "        attention_probs_dropout_prob=0.0,\n",
    "        max_position_embeddings=eval_cfg.max_position_embeddings,\n",
    "        label_smoothing=0.1,\n",
    "        input_mask_from_dataset=True,\n",
    "        beam_width=4,\n",
    "        max_decode_length=80,\n",
    "        length_penalty_weight=1.0,\n",
    "        dtype=mstype.float32,\n",
    "        compute_type=mstype.float16)\n",
    "else:\n",
    "    raise Exception(\"The src/eval_confige of transformer_network must base or large and same with the train_confige confige. Change the str/eval_confige file and try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. 定义评估测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerInferCell(nn.Cell):\n",
    "    \"\"\"\n",
    "    Encapsulation class of transformer network infer.\n",
    "    \"\"\"\n",
    "    def __init__(self, network):\n",
    "        super(TransformerInferCell, self).__init__(auto_prefix=False)\n",
    "        self.network = network\n",
    "\n",
    "    def construct(self,\n",
    "                  source_ids,\n",
    "                  source_mask):\n",
    "        predicted_ids = self.network(source_ids, source_mask)\n",
    "        return predicted_ids\n",
    "\n",
    "def load_weights(model_path):\n",
    "    \"\"\"\n",
    "    Load checkpoint as parameter dict, support both npz file and mindspore checkpoint file.\n",
    "    \"\"\"\n",
    "    if model_path.endswith(\".npz\"):\n",
    "        ms_ckpt = np.load(model_path)\n",
    "        is_npz = True\n",
    "    else:\n",
    "        ms_ckpt = load_checkpoint(model_path)\n",
    "        is_npz = False\n",
    "\n",
    "    weights = {}\n",
    "    for msname in ms_ckpt:\n",
    "        infer_name = msname\n",
    "        if \"tfm_decoder\" in msname:\n",
    "            infer_name = \"tfm_decoder.decoder.\" + infer_name\n",
    "        if is_npz:\n",
    "            weights[infer_name] = ms_ckpt[msname]\n",
    "        else:\n",
    "            weights[infer_name] = ms_ckpt[msname].data.asnumpy()\n",
    "    weights[\"tfm_decoder.decoder.tfm_embedding_lookup.embedding_table\"] = \\\n",
    "        weights[\"tfm_embedding_lookup.embedding_table\"]\n",
    "\n",
    "    parameter_dict = {}\n",
    "    for name in weights:\n",
    "        parameter_dict[name] = Parameter(Tensor(weights[name]), name=name)\n",
    "    return parameter_dict\n",
    "\n",
    "def evaluate(cfg):\n",
    "    \"\"\"\n",
    "    Transformer evaluation.\n",
    "    \"\"\"\n",
    "    context.set_context(mode=context.GRAPH_MODE, device_target=\"Ascend\", reserve_class_name_in_scope=False)\n",
    "\n",
    "    tfm_model = TransformerModel(config=transformer_net_cfg, is_training=False, use_one_hot_embeddings=False)\n",
    "    print(cfg.model_file)\n",
    "    parameter_dict = load_weights(cfg.model_file)\n",
    "    load_param_into_net(tfm_model, parameter_dict)\n",
    "    tfm_infer = TransformerInferCell(tfm_model)\n",
    "    model = Model(tfm_infer)\n",
    "    \n",
    "    tokenizer = tokenization.WhiteSpaceTokenizer(vocab_file=cfg.vocab_file)\n",
    "    dataset = load_dataset(batch_size=cfg.batch_size, data_file=cfg.data_file)\n",
    "    predictions = []\n",
    "    source_sents = []\n",
    "    target_sents = []\n",
    "    f2 = open(cfg.test_source_file, 'r', encoding='utf-8')\n",
    "    for batch in dataset.create_dict_iterator():\n",
    "        source_sents.append(batch[\"source_eos_ids\"])\n",
    "        target_sents.append(batch[\"target_eos_ids\"])\n",
    "        source_ids = Tensor(batch[\"source_eos_ids\"], mstype.int32)\n",
    "        source_mask = Tensor(batch[\"source_eos_mask\"], mstype.int32)\n",
    "        predicted_ids = model.predict(source_ids, source_mask)\n",
    "        #predictions.append(predicted_ids.asnumpy())\n",
    "        # ----------------------------------------decode and write to file(token file)---------------------\n",
    "        batch_out = predicted_ids.asnumpy()\n",
    "        for i in range(transformer_net_cfg.batch_size):\n",
    "            if batch_out.ndim == 3:\n",
    "                batch_out = batch_out[:, 0]\n",
    "            token_ids = [str(x) for x in batch_out[i].tolist()]\n",
    "            token=\" \".join(token_ids)\n",
    "            #-------------------------------token_ids to real output file-------------------------------\n",
    "            token_ids = [int(x) for x in token.strip().split()]\n",
    "            tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "            sent = \" \".join(tokens)\n",
    "            sent = sent.split(\"<s>\")[-1]\n",
    "            sent = sent.split(\"</s>\")[0]\n",
    "            \n",
    "            label_sent = f2.readline().strip()+'\\t'\n",
    "            print(\"source: {}\".format(label_sent))\n",
    "            print(\"result: {}\".format(sent.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. 启动评估测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "evaluate(eval_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mindspore-python3.7-aarch64",
   "language": "python",
   "name": "mindspore-python3.7-aarch64"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
